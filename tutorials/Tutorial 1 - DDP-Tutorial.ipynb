{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/pnnl/DeepDataProfiler.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/vgg_profiling.png\" alt=\"VGG Profiling Image\" style=\"width: 400px;float: right;margin: 10pt\"/>\n",
    "\n",
    "# Deep Data Profiler(DDP)\n",
    "\n",
    "DDP characterizes both the data and the model, pointing to the most influential neurons and weights used for classification and identifying the inter-relationships between neurons in the profiled layers. DDP is currently implemented for Pytorch modules: \n",
    "- torch.nn.Linear,\n",
    "- torch.nn.MaxPool2d,\n",
    "- torch.nn.AdaptiveAvgPool2d,\n",
    "- torch.nn.Conv2d\n",
    "\n",
    "Using these modules, DDP can construct profiles for sequential (VGG-like) and Resnet architectures.\n",
    "\n",
    "### Important Terms\n",
    "- Layers - collections of modules across the model. We group weighted modules with their activations and treat these as a single layer. Pooling modules are given their own layers as they affect the indices of contributing neurons.\n",
    "- Neuron - the index of an input or output tensor for each layer, we identify the most influential neurons\n",
    "    - A neuron can refer to either a single element or a channel of a tensor, depending on the profile.\n",
    "- Synapse - a tuple of input and output neurons (and weight indices where appropriate) within a layer\n",
    "- Profile instance - the critical set of neurons and synapses (up to a threshold value) that leads to a final predicted class\n",
    "- Threshold - a parameter to determine the neurons and synapses used for the profile\n",
    "- Class Profile - an aggregation of profile instances for inputs from the same class\n",
    "- Jaccard metrics \n",
    "    - Computes similarity between profiles\n",
    "    - Can cluster sets of profiles\n",
    "\n",
    "### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "sys.path.append('..')\n",
    "import deep_data_profiler as ddp\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import torchvision.models.vgg as vgg\n",
    "import torchvision.models.resnet as resnet\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To ensure reproducibility we set the pytorch seed. This only works if the versioning of libraries is held\n",
    "## constant\n",
    "torch.manual_seed(0)\n",
    "## We will also specify the device. The profiling code will run on cpu or cuda.\n",
    "device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src=\"images/eagleimage.png\" alt=\"VGG Profiling Image\" style=\"width: 175px;float: right;margin: 15pt\"/>\n",
    "\n",
    "# How do we compute a profile?\n",
    "\n",
    "\n",
    "Profiling requires a pretrained model and data that mirrors the training data. In this example the model is the pretrained\n",
    "Pytorch VGG16 model. We create a profiler for the data by instantiating the class <code>ddp.TorchProfiler</code> with the \n",
    "pretrained model.\n",
    "\n",
    "We preprocess the eagle image on the right into Pytorch tensors suitable as input for the VGG16 model. To create a \n",
    "profile for these images we then use the <code>ddp.TorchProfiler.create_profile()</code> method on the image.\n",
    "\n",
    "## 1. Generate the data\n",
    "We preprocess an eagle image from Imagenet by applying the transformations that were used on the training set (i.e. normalizing, resizing, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment the following to run locally\n",
    "imagepath = 'images/n01614925/ILSVRC2012_val_00046632.JPEG'\n",
    "img = cv2.imread(imagepath)\n",
    "\n",
    "### Uncomment the following to run in Colab\n",
    "# import requests\n",
    "# imagepath = \"https://github.com/pnnl/DeepDataProfiler/blob/develop/tutorials/images/n01614925/ILSVRC2012_val_00046632.JPEG\"\n",
    "# resp = requests.get(imagepath, stream=True).raw\n",
    "# img = np.asarray(bytearray(resp.read()), dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8507, -0.8507, -0.8335,  ..., -1.0562, -1.0733, -0.9877],\n",
       "          [-0.9192, -0.8678, -0.7822,  ..., -1.0390, -1.0733, -0.9877],\n",
       "          [-0.8335, -0.8335, -0.8678,  ..., -0.9877, -1.0390, -1.0562],\n",
       "          ...,\n",
       "          [ 0.7419,  0.0569, -0.6452,  ..., -1.0562, -1.0562, -1.1247],\n",
       "          [ 0.6049, -0.0801, -0.7308,  ..., -1.0562, -1.0562, -1.0904],\n",
       "          [ 0.5878, -0.0287, -0.6452,  ..., -1.0048, -1.1247, -1.1247]],\n",
       "\n",
       "         [[-0.7927, -0.7927, -0.7227,  ..., -0.9503, -0.9328, -0.9503],\n",
       "          [-0.7752, -0.7577, -0.8803,  ..., -0.9853, -0.9678, -0.9328],\n",
       "          [-0.8277, -0.7752, -0.8978,  ..., -0.9328, -0.9503, -0.9678],\n",
       "          ...,\n",
       "          [ 0.8179,  0.0476, -0.5476,  ..., -0.9503, -0.9678, -1.0028],\n",
       "          [ 0.7304,  0.1001, -0.5476,  ..., -1.0378, -0.9503, -0.9503],\n",
       "          [ 0.6429,  0.0476, -0.6352,  ..., -0.9853, -1.0378, -0.9853]],\n",
       "\n",
       "         [[-0.6541, -0.7238, -0.6715,  ..., -0.7936, -0.7761, -0.7587],\n",
       "          [-0.6541, -0.6715, -0.7238,  ..., -0.8284, -0.8633, -0.8284],\n",
       "          [-0.6715, -0.6367, -0.7238,  ..., -0.7936, -0.8284, -0.8284],\n",
       "          ...,\n",
       "          [ 0.8622,  0.2173, -0.4450,  ..., -0.8284, -0.8284, -0.8807],\n",
       "          [ 0.8099,  0.1651, -0.3927,  ..., -0.8633, -0.8633, -0.8807],\n",
       "          [ 0.7228,  0.1302, -0.4624,  ..., -0.8458, -0.8981, -0.8633]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagepath = \"https://github.com/pnnl/DeepDataProfiler/blob/develop/tutorials/images/n01614925/ILSVRC2012_val_00046632.JPEG\"\n",
    "# imagepath = 'images/n01614925/ILSVRC2012_val_00046632.JPEG'\n",
    "def torch_image(img):\n",
    "    \"\"\"normalize the values and resize for processing\"\"\"\n",
    "    norm=((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    normalize = transforms.Compose([transforms.Normalize(mean=norm[0], std=norm[1])])\n",
    "    # img = cv2.imread(imgpath)\n",
    "    img = np.expand_dims(img, 2).repeat(3, axis=2) if len(img.shape) == 2 else img\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = to_tensor(img)\n",
    "    return torch.unsqueeze(normalize(img),0)\n",
    "\n",
    "timage = torch_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose a model\n",
    "\n",
    "We create an instance of the `ddp.TorchProfiler` class, which allows us to use all of the DDP profiling methods for the model. We will highlight some of these methods, including:\n",
    "\n",
    "- `available_modules()` : returns a list of available module names and associated models\n",
    "- `create_layers()` : creates a dictionary of layers to profile\n",
    "- `create_profile()` : generates a profile for a single input as it passes through the layers\n",
    "- `dict_view()` : reformats the profile using dictionaries to represent the counts and weights\n",
    "\n",
    "Here we instantiate the PyTorch VGG-16 model and its DDP Profiler, `profiler`, with a threshold.\n",
    "\n",
    "Uncomment the Resnet code to see the tutorial with ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.1\n",
    "model = vgg.vgg16(pretrained=True).eval()\n",
    "# model = resnet.resnet18(pretrained=True).eval()\n",
    "profiler = ddp.TorchProfiler(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decide which layers to profile. \n",
    "A profile represents connections between the layers of the network, and we may not always want to profile all of the layers. First let's take a look at the available modules in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profiler.model.available_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For profiling, layers are weighted modules or pooling modules. We construct a layer dictionary to pass to the profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note the helper function referenced in each layer start with _contrib and \n",
    "## refer to the type of Pytorch layer being profiled\n",
    "layerdict = profiler.create_layers();layerdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will profile <b>all layers</b> of the model. To profile a subset of the layers, pass either a list of layers $[L_1, L_2, \\dots, L_n]$ or a tuple-formatted range $(L_1, L_n) = [L_1, L_2, \\dots, L_{n-1}]$ to <code>TorchProfiler.create_profile()</code> as the <code>layers_to_profile</code> argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate the Profile\n",
    "**The primary purpose of a `ddp.TorchProfiler` object is to generate `ddp.Profile` objects for input data as it passes through the layers of a model.**\n",
    "\n",
    "We compute both a channel-wise and element-wise profile of the image, thresholding at 10%. This gives us two profiles that have been constructed using different definitions of the neuron, the significance of which we will discuss in the next section.\n",
    "\n",
    "<b>Note:</b> You can experiment with different thresholds, though high thresholds will take longer to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "channel_profile = profiler.create_profile(timage,\n",
    "                                          layerdict,\n",
    "                                          threshold=0.1,\n",
    "                                          channels=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "element_profile = profiler.create_profile(timage,\n",
    "                                          layerdict,\n",
    "                                          threshold=0.1,\n",
    "                                          channels=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What is a neuron?\n",
    "\n",
    "There are two different ways we can use neurons to track influence across the activations of a CNN. A \"neuron\" can refer to either a single channel of a convolutional layer, or to a single element. The choice of element or channel definition of neurons is consistent throughout a single profile.\n",
    "\n",
    "<center><img src=\"images/synapsetypes.png\" alt=\"Synapse Types\" style=\"width: 800px;margin: 10pt\"/></center>\n",
    "\n",
    " Each definition has its own strengths:\n",
    " - **Channel-wise** profiles are faster to compute, but lack granularity.\n",
    " - **Element-wise** profiles provide a more granular view of the network, but take require much more computing time and power.\n",
    " \n",
    "There are some slight differences in the sparse matrix representations for the different neuron types, so both channel-wise and element-wise examples will be provided.\n",
    " \n",
    "<b>Note</b>: The multiple definitions of *neuron* only apply to convolutional layers; the elements of a fully-connected linear layer are always treated as individual neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What is in a Profile Object?\n",
    "\n",
    "Let's look at a profile of a single image in the eagles class. We use the profile to find the largest contributors among the neurons and weights that lead to the final classification. Each profile contains python dictionaries of neuron_counts, synapse_counts, and synapse_weights, listing the significant contributors for the layers profiled.\n",
    "Each profile has aggregation metrics of total and size. Profiles may be added together to form a new profile or added in place.\n",
    "\n",
    "- <code>neuron_counts</code> : dictionary of sparse matrices, representing the number of significant synapses each neuron participates in at each layer.\n",
    "- <code>neuron_weights</code> : dictionary of sparse matrices, representing the weight of influential neurons at each layer\n",
    "- <code>synapse counts</code> : dictionary of sparse matrices, representing to the counts of connections between input and output neurons through each layer\n",
    "- <code>synapse weights</code> :  dictionary of sparse matrices, representing the weighted connections between input and output neurons through each layer\n",
    "- <code>num_inputs</code> : int, the number of input images a profile is constructed from (i.e. if you add two single image profiles with num_inputs = 1, the resulting aggregate profile will have num_inputs = 2\n",
    "- <code>neuron_type</code> : string (optional), either 'channel' for a channel-wise profile, 'neuron' for a neuron-wise profile, or 'mixed' for an aggregate of profiles with mismatched types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful method for examining an existing profile is <code>ddp.TorchProfiler.dict_view()</code>, which takes a standard Profile object as input, and returns a new Profile object where dictionaries are used to represent the counts and weights data originally held by sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_view returns a dummy Profile object that holds dictionary representations of each count and weight\n",
    "channel_profile_dict = profiler.dict_view(channel_profile)\n",
    "element_profile_dict = profiler.dict_view(element_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main cases where the dictionary view is important:\n",
    "- Examining the synapses in a profile on Resnet architecture, where the preceding layer to layer $L$ is not necessarily the layer indexed $L-1$\n",
    "- Using the <code>algorithms/homology</code> package to create a Profile Graph, where the dictionary view is required to construct the edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Counts: \n",
    "Note that each layer points at a sparse matrix representing the contributing neurons and the number of times each contributes within the layer. In this example we used a threshold of 0.1, meaning for each contributing neuron $n$, we searched for the smallest set of neurons $\\{n_\\alpha\\}$ in the previous layer which would contribute to $10\\%$ of the value of the neuron at $n$.\n",
    "\n",
    "We refer to the dimensions of the layer activation tensor as $\\text{num_channels}, \\text{height}, $ and $\\text{width}$.\n",
    "\n",
    "In a **channel-wise** profile, neuron counts are shaped like a column vector of length $\\text{num_channels}$. The entry at index $c$ gives the number of times channel $c$ has been identified as either influential within its layer, or as a contributor to an influential neuron in the following layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neuron counts of an early convolutional layer (4) with 128 channels\n",
    "## the first index is 0 because the counts are a column vector\n",
    "print(channel_profile.neuron_counts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an **element-wise** profile, neuron counts are a sparse matrix with shape: $\\text{num_channels} \\times (\\text{height})(\\text{width})$. The entry at index $c,i$ gives the number of times the element located at position $i$, a flattened spatial index of channel $c$, has been identified as either influential within its layer, or as a contributor to an influential neuron in the following layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element_profile.neuron_counts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although these are two distinct profiles that identify different contributors, they represent two ways of describing the same model-data pairing. As such, there are several similarities between them to take note of. More than half of the channel indices from each set of neuron counts can be found in both sets, and both of the neurons with counts greater than one in the element-wise profile correspond to a channel with counts greater than one in the channel-wise profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_channels = ({elem for elem in channel_profile.neuron_counts[4].col}\n",
    "                   & {elem for elem in element_profile.neuron_counts[4].row})\n",
    "print(f'Shared channels between the profiles: {shared_channels}')\n",
    "shared_ratio = len(shared_channels)/len(element_profile_dict.neuron_counts[4])\n",
    "print(f'{\"{:.2%}\".format(shared_ratio)} of channel indices in the element-wise neuron counts are shared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Weights\n",
    "Unlike neuron counts, which are incremented equally whether a neuron is a contributor to an influential neuron or is influential itself, neuron weights are assigned only to the neurons that are identified as influential in their layer. Neuron weights have the same shape as neuron counts, but the values represent their relative importance among all neurons in that layer. \n",
    "\n",
    "For our example with a threshold of 0.1, we considered the maximum activation from each channel as our set of neurons and chose the top $10\\%$ as influential. This method is similar to the method of identifying contributors; we can think of the neurons in terms of their contribution to a percentage of the total sum of maximum activations from each channel.\n",
    "\n",
    "In a **channel-wise** profile, the entry at index $c$ gives the weight of channel $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(channel_profile.neuron_weights[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights sum to 1\n",
    "print(f'Sum of weights: {np.sum(channel_profile.neuron_weights[4].data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an **element-wise** profile, the entry at index $c,i$ gives the weight of the element located at position $i$, a flattened spatial index of channel $c$. If the weight is greater than zero, then $i$ is the position of the maximum activation in channel $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element_profile.neuron_weights[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for each influential channel in the channel-wise profile there is a corresponding influential element within the same channel with the same weight in the element-wise profile. This is because the method of identifying influential neurons from strong activations is the same between element-wise and channel-wise profiles, but the influential neuron is represented with higher granularity in the element-wise profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synapse Counts\n",
    "The synapse counts are represented by a sparse matrix that acts as an adjacency matrix between two layers. For a single image profile all entries will be either zero or one. In a class profile a count $n$, $n > 1$ means that a synapse has been present in $n$ of the image profiles aggregated to form the class profile.\n",
    "\n",
    "In a **channel-wise** profile, the synapse counts for layer $L$ are a $\\text{nchannels}_L \\times \\text{nchannels}_{L-1}$ matrix, where a one in entry $i,j$ represents a synapse between channel $j$ in layer $L-1$ and channel $i$ in layer $L$. In other words, channel $j$ has been identified as a contributor to the influential channel $i$.\n",
    "\n",
    "Note: in Resnet architectures, the layer contributing to layer $L$ may not always be indexed as layer $L-1$, but we generalize $L-1$ here to signify the relevant preceding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 4 has 128 channels, layer 3 has 64 channels\n",
    "channel_profile.synapse_counts[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first column represents the influential endpoints of the synapses (in L)\n",
    "# the second column represents the contributing startpoints of the synapses (in L-1)\n",
    "# Ex: there is a synapse between channel 0 of layer 3 and channel 46 of layer 4\n",
    "print(channel_profile.synapse_counts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary view gives additonal layer information, which can be especially useful in Resnet architectures where contributors to layer $L$ may not necessarily come from the layer with index $L-1$. It also reorders the synapse pairs so that the contributor comes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_profile_dict.synapse_counts[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an **element-wise** profile, the synapse counts for convolutional layer $L$ are stored as a $(\\text{nchannels}_L) (\\text{height}_L)(\\text{width}_L) \\times (\\text{nchannels}_{L-1})(\\text{height}_{L-1})(\\text{width}_{L-1})$ sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element_profile.synapse_counts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synapse Weights\n",
    "Synapse weights are sparse matrices with the same shape as the corresponding synapse counts for the layer. While synapse counts describe which synapses exist between two layers, synapse weights describe the weights assigned to those synapses. With a threshold of 0.1, the weight of a synapse is the proportion of $10\\%$ of the influential neuron's value that is provided by the value of the contributing neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_profile_dict.synapse_weights[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_profile_dict.synapse_weights[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
